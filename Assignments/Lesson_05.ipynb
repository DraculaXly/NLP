{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Tree -> Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from smart_open import open\n",
    "from collections import defaultdict\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = r'D:\\Github\\Data\\sqlResult_1558435.csv'\n",
    "content = pd.read_csv(csv_path, encoding='gb18030')\n",
    "content = content.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "      <th>feature</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89617</td>\n",
       "      <td></td>\n",
       "      <td>快科技@http://www.kkj.cn/</td>\n",
       "      <td>此外，自本周（6月12日）起，除小米手机6等15款机型外，其余机型已暂停更新发布（含开发版/...</td>\n",
       "      <td>{\"type\":\"科技\",\"site\":\"cnbeta\",\"commentNum\":\"37\"...</td>\n",
       "      <td>小米MIUI 9首批机型曝光：共计15款</td>\n",
       "      <td>http://www.cnbeta.com/articles/tech/623597.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89616</td>\n",
       "      <td></td>\n",
       "      <td>快科技@http://www.kkj.cn/</td>\n",
       "      <td>骁龙835作为唯一通过Windows 10桌面平台认证的ARM处理器，高通强调，不会因为只考...</td>\n",
       "      <td>{\"type\":\"科技\",\"site\":\"cnbeta\",\"commentNum\":\"15\"...</td>\n",
       "      <td>骁龙835在Windows 10上的性能表现有望改善</td>\n",
       "      <td>http://www.cnbeta.com/articles/tech/623599.htm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id author                  source  \\\n",
       "0  89617         快科技@http://www.kkj.cn/   \n",
       "1  89616         快科技@http://www.kkj.cn/   \n",
       "\n",
       "                                             content  \\\n",
       "0  此外，自本周（6月12日）起，除小米手机6等15款机型外，其余机型已暂停更新发布（含开发版/...   \n",
       "1  骁龙835作为唯一通过Windows 10桌面平台认证的ARM处理器，高通强调，不会因为只考...   \n",
       "\n",
       "                                             feature  \\\n",
       "0  {\"type\":\"科技\",\"site\":\"cnbeta\",\"commentNum\":\"37\"...   \n",
       "1  {\"type\":\"科技\",\"site\":\"cnbeta\",\"commentNum\":\"15\"...   \n",
       "\n",
       "                        title                                             url  \n",
       "0        小米MIUI 9首批机型曝光：共计15款  http://www.cnbeta.com/articles/tech/623597.htm  \n",
       "1  骁龙835在Windows 10上的性能表现有望改善  http://www.cnbeta.com/articles/tech/623599.htm  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content = content['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string):\n",
    "    return ' '.join(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\qq706\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.805 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'这是 一个 测试'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut('这是一个测试')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/2/library/re.html\n",
    "# []: Used to indicate a set of characters\n",
    "# \\d: this is equivalent to the set [0-9]\n",
    "# \\w: this is equivalent to the set [a-zA-Z0-9_]\n",
    "# +: match 1 or more repetitions of the preceding RE\n",
    "def token(string):\n",
    "    return re.findall(r'[\\d|\\w]+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['这是一个测试', '0000_abdc']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token('这是一个测试\\n\\n\\n0000_abdc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content = [token(n) for n in news_content] #clean data\n",
    "news_content = [' '.join(n) for n in news_content] #join data\n",
    "news_content = [cut(n) for n in news_content] #cut data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'此外   自 本周   6 月 12 日   起   除 小米 手机 6 等 15 款 机型 外   其余 机型 已 暂停 更新 发布   含 开发 版   体验版 内测   稳定版 暂不受 影响   以 确保 工程师 可以 集中 全部 精力 进行 系统优化 工作   有人 猜测 这 也 是 将 精力 主要 用到 MIUI   9 的 研发 之中   MIUI   8 去年 5 月 发布   距今已有 一年 有余   也 是 时候 更新换代 了   当然   关于 MIUI   9 的 确切 信息   我们 还是 等待 官方消息'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"D:\\Github\\Data\\news-sentences-cut.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'w', encoding='utf-8') as f:\n",
    "    for n in news_content:\n",
    "        f.write(n + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "news_word2vec= Word2Vec(LineSentence(file_path), size=35, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "news_word2vec.save(r\"D:\\Github\\Data\\news-sentences-cut.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('克罗地亚', 0.8639040589332581),\n",
       " ('比什凯克', 0.8350920677185059),\n",
       " ('意大利', 0.8338714838027954),\n",
       " ('捷克', 0.8329738974571228),\n",
       " ('比利时', 0.8176416754722595),\n",
       " ('西班牙', 0.8048977851867676),\n",
       " ('丹麦', 0.80287766456604),\n",
       " ('乌拉圭', 0.7993817329406738),\n",
       " ('摩洛哥', 0.7928330302238464),\n",
       " ('中国香港', 0.7898896336555481),\n",
       " ('奥地利', 0.7846331596374512),\n",
       " ('科特迪瓦', 0.7831850647926331),\n",
       " ('苏格兰', 0.7816621661186218),\n",
       " ('罗马尼亚', 0.7733701467514038),\n",
       " ('里斯本', 0.7687420845031738),\n",
       " ('巴塞罗那', 0.7680408954620361),\n",
       " ('瑞士', 0.7672402858734131),\n",
       " ('巴拉圭', 0.761271595954895),\n",
       " ('马德里', 0.7551348805427551),\n",
       " ('斯洛文尼亚', 0.754212498664856)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_word2vec = Word2Vec.load(r\"D:\\Github\\Data\\news-sentences-cut.model\")\n",
    "news_word2vec.wv.most_similar('葡萄牙', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Date, Better Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 分词的问题\n",
    "2. **数据量**，数据越多，效果越好，维基百科加进来，那么同义词就要好很多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('表示', 0.8957445621490479),\n",
       " ('指出', 0.8473508358001709),\n",
       " ('认为', 0.8381446599960327),\n",
       " ('看来', 0.8200075030326843),\n",
       " ('坦言', 0.8185017108917236),\n",
       " ('告诉', 0.7958028316497803),\n",
       " ('介绍', 0.7709847092628479),\n",
       " ('明说', 0.7574785947799683),\n",
       " ('透露', 0.7406556010246277),\n",
       " ('称', 0.7219231128692627)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_word2vec.most_similar('说', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#进行存储\n",
    "def memo(func):\n",
    "    cache = {} \n",
    "    @wraps(func)\n",
    "    def _wrap(n, *arg):\n",
    "        for i in n:\n",
    "            if i in cache: result = cache[i] \n",
    "            else:\n",
    "                result = func(i, *arg) \n",
    "                cache[i] = result \n",
    "            return result\n",
    "    return _wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo\n",
    "def get_related_words(initial_words, model):\n",
    "    \"\"\"\n",
    "    @initial_words are initial words we already know\n",
    "    @model is the word2vec model\n",
    "    \"\"\"\n",
    "    unseen = initial_words\n",
    "    \n",
    "    seen = defaultdict(int)\n",
    "    \n",
    "    max_size = 500  # could be greater\n",
    "    \n",
    "    while unseen and len(seen) < max_size:\n",
    "        #if len(seen) % 50 == 0: \n",
    "            #print('seen length : {}'.format(len(seen)))\n",
    "            \n",
    "        node = unseen.pop(0)\n",
    "        \n",
    "        new_expanding = [w for w, s in model.wv.most_similar(node, topn=20)]\n",
    "        \n",
    "        unseen += new_expanding\n",
    "        \n",
    "        seen[node] += 1\n",
    "        \n",
    "        # optimal: 1. score function could be revised\n",
    "        # optimal: 2. using dymanic programming to reduce computing time\n",
    "    \n",
    "    return seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97927"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'pop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-121c457c57d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrelated_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_related_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'说'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'表示'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnews_word2vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-91-ff0ca2b6ce26>\u001b[0m in \u001b[0;36m_wrap\u001b[1;34m(n, *arg)\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m                 \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-92-8102d96be755>\u001b[0m in \u001b[0;36mget_related_words\u001b[1;34m(initial_words, model)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;31m#print('seen length : {}'.format(len(seen)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munseen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mnew_expanding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'pop'"
     ]
    }
   ],
   "source": [
    "related_words = get_related_words(['说', '表示'], news_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('说', 90),\n",
       " ('指出', 87),\n",
       " ('坦言', 83),\n",
       " ('表示', 82),\n",
       " ('认为', 78),\n",
       " ('透露', 75),\n",
       " ('所说', 71),\n",
       " ('看来', 70),\n",
       " ('特别强调', 64),\n",
       " ('告诉', 61),\n",
       " ('明说', 57),\n",
       " ('介绍', 49),\n",
       " ('称', 44),\n",
       " ('提到', 44),\n",
       " ('强调', 39),\n",
       " ('中说', 37),\n",
       " ('文说', 34),\n",
       " ('普遍认为', 32),\n",
       " ('建议', 29),\n",
       " ('相信', 29),\n",
       " ('对此', 28),\n",
       " ('提及', 24),\n",
       " ('写道', 24),\n",
       " ('问', 23),\n",
       " ('直言', 23),\n",
       " ('现阶段', 21),\n",
       " ('确信', 20),\n",
       " ('称赞', 19),\n",
       " ('看好', 19),\n",
       " ('觉得', 19),\n",
       " ('而言', 17),\n",
       " ('中称', 17),\n",
       " ('知情', 17),\n",
       " ('地说', 16),\n",
       " ('接受', 16),\n",
       " ('证实', 15),\n",
       " ('时说', 14),\n",
       " ('声称', 14),\n",
       " ('原话', 14),\n",
       " ('回答', 14),\n",
       " ('事实上', 13),\n",
       " ('表明', 13),\n",
       " ('承认', 13),\n",
       " ('说道', 12),\n",
       " ('列举', 12),\n",
       " ('反复强调', 12),\n",
       " ('充分说明', 12),\n",
       " ('坦承', 11),\n",
       " ('看到', 11),\n",
       " ('引用', 11),\n",
       " ('描述', 11),\n",
       " ('问道', 11),\n",
       " ('解释', 11),\n",
       " ('坚信', 11),\n",
       " ('具名', 10),\n",
       " ('怼', 10),\n",
       " ('批评', 10),\n",
       " ('肯定', 9),\n",
       " ('黄超', 9),\n",
       " ('武说', 9),\n",
       " ('还称', 9),\n",
       " ('援引', 9),\n",
       " ('猜测', 9),\n",
       " ('反驳', 9),\n",
       " ('赞赏', 9),\n",
       " ('谈论', 9),\n",
       " ('当然', 9),\n",
       " ('伟说', 8),\n",
       " ('来说', 8),\n",
       " ('童岚', 8),\n",
       " ('谢琳', 8),\n",
       " ('许臻', 8),\n",
       " ('祁', 8),\n",
       " ('深有感触', 8),\n",
       " ('农办', 8),\n",
       " ('深有体会', 8),\n",
       " ('董', 8),\n",
       " ('纳说', 8),\n",
       " ('非常重视', 8),\n",
       " ('请问', 8),\n",
       " ('质疑', 8),\n",
       " ('眼中', 7),\n",
       " ('留意到', 7),\n",
       " ('见到', 7),\n",
       " ('康逸', 7),\n",
       " ('冯俊伟摄', 7),\n",
       " ('自称', 7),\n",
       " ('获悉', 7),\n",
       " ('了解', 7),\n",
       " ('供图', 7),\n",
       " ('援外', 7),\n",
       " ('吴越', 7),\n",
       " ('肖凤祥', 7),\n",
       " ('卫生厅', 7),\n",
       " ('负责人', 7),\n",
       " ('正如', 7),\n",
       " ('的话', 7),\n",
       " ('此行', 7),\n",
       " ('爱台', 7),\n",
       " ('时称', 7),\n",
       " ('检视', 7),\n",
       " ('十分重视', 7),\n",
       " ('远远不够', 7),\n",
       " ('最缺', 7),\n",
       " ('归功于', 7),\n",
       " ('王耀', 7),\n",
       " ('陈说', 7),\n",
       " ('看准', 7),\n",
       " ('推脱', 7),\n",
       " ('要说', 7),\n",
       " ('给出', 7),\n",
       " ('答', 6),\n",
       " ('早就', 6),\n",
       " ('西万', 6),\n",
       " ('黄进', 6),\n",
       " ('浠水县', 6),\n",
       " ('保国', 6),\n",
       " ('农艺师', 6),\n",
       " ('海涛', 6),\n",
       " ('红霞', 6),\n",
       " ('张晓明', 6),\n",
       " ('民法学', 6),\n",
       " ('对瓦姆', 6),\n",
       " ('感叹', 6),\n",
       " ('佩服', 6),\n",
       " ('澄清', 6),\n",
       " ('暗示', 6),\n",
       " ('说法', 5),\n",
       " ('墨方', 5),\n",
       " ('据称', 5),\n",
       " ('曾称', 5),\n",
       " ('美国中央情报局', 5),\n",
       " ('上称', 5),\n",
       " ('重申', 5),\n",
       " ('高度重视', 5),\n",
       " ('提出', 5),\n",
       " ('着重强调', 5),\n",
       " ('一贯', 5),\n",
       " ('全力支持', 5),\n",
       " ('呼吁', 5),\n",
       " ('宣示', 5),\n",
       " ('本着', 5),\n",
       " ('敦促', 5),\n",
       " ('陆慷说', 5),\n",
       " ('口中', 5),\n",
       " ('慎重', 5),\n",
       " ('回应', 5),\n",
       " ('对于', 5),\n",
       " ('有何', 5),\n",
       " ('预言', 5),\n",
       " ('担忧', 5),\n",
       " ('期望', 5),\n",
       " ('或许', 5),\n",
       " ('展望未来', 5),\n",
       " ('显然', 5),\n",
       " ('其实', 5),\n",
       " ('因此', 5),\n",
       " ('努钦称', 4),\n",
       " ('里亚布', 4),\n",
       " ('核导弹', 4),\n",
       " ('申明', 4),\n",
       " ('评述', 4),\n",
       " ('简短', 4),\n",
       " ('为什么', 4),\n",
       " ('感慨', 4),\n",
       " ('心里', 4),\n",
       " ('那位', 4),\n",
       " ('要求', 4),\n",
       " ('意见建议', 4),\n",
       " ('必要', 4),\n",
       " ('还应', 4),\n",
       " ('相应', 4),\n",
       " ('吴恒', 4),\n",
       " ('提醒', 4),\n",
       " ('指南', 4),\n",
       " ('党政部门', 4),\n",
       " ('做法', 4),\n",
       " ('评价', 4),\n",
       " ('加以', 4),\n",
       " ('专家建议', 4),\n",
       " ('赞同', 4),\n",
       " ('黄向', 4),\n",
       " ('动向', 4),\n",
       " ('刁', 4),\n",
       " ('鉴于', 4),\n",
       " ('对', 4),\n",
       " ('知道', 4),\n",
       " ('阐述', 4),\n",
       " ('清楚', 4),\n",
       " ('近来', 4),\n",
       " ('热议', 4),\n",
       " ('关注', 4),\n",
       " ('预料', 4),\n",
       " ('令', 4),\n",
       " ('招致', 4),\n",
       " ('说明', 4),\n",
       " ('指望', 4),\n",
       " ('但是', 4),\n",
       " ('依赖于', 4),\n",
       " ('毕竟', 4),\n",
       " ('意料', 4),\n",
       " ('没错', 4),\n",
       " ('本来', 4),\n",
       " ('谈谈', 3),\n",
       " ('反问', 3),\n",
       " ('提问', 3),\n",
       " ('注意', 3),\n",
       " ('共见', 3),\n",
       " ('追问', 3),\n",
       " ('吗', 3),\n",
       " ('谈', 3),\n",
       " ('南方日报', 3),\n",
       " ('听', 3),\n",
       " ('感激', 3),\n",
       " ('眼里', 3),\n",
       " ('开玩笑', 3),\n",
       " ('由衷', 3),\n",
       " ('如是说', 3),\n",
       " ('考量', 3),\n",
       " ('希望', 3),\n",
       " ('期待', 3),\n",
       " ('将来', 3),\n",
       " ('让', 3),\n",
       " ('谈到', 3),\n",
       " ('想法', 3),\n",
       " ('理解', 3),\n",
       " ('之门', 3),\n",
       " ('来讲', 3),\n",
       " ('本身', 3),\n",
       " ('只是', 3),\n",
       " ('是因为', 3),\n",
       " ('实际上', 3),\n",
       " ('衡量', 3),\n",
       " ('不过', 3),\n",
       " ('之所以', 3),\n",
       " ('低估', 3),\n",
       " ('问过', 3),\n",
       " ('托说', 2),\n",
       " ('三军', 2),\n",
       " ('德说', 2),\n",
       " ('王晔', 2),\n",
       " ('科说', 2),\n",
       " ('政务司', 2),\n",
       " ('驻京办', 2),\n",
       " ('常务副', 2),\n",
       " ('军事科学院', 2),\n",
       " ('副', 2),\n",
       " ('锦', 2),\n",
       " ('韩长', 2),\n",
       " ('发展部', 2),\n",
       " ('斯克', 2),\n",
       " ('谈起', 2),\n",
       " ('溢于言表', 2),\n",
       " ('说起', 2),\n",
       " ('倍感', 2),\n",
       " ('感触', 2),\n",
       " ('总是', 2),\n",
       " ('太好了', 2),\n",
       " ('思乡', 2),\n",
       " ('获知', 2),\n",
       " ('胜任', 2),\n",
       " ('智囊团', 2),\n",
       " ('不得而知', 2),\n",
       " ('秘而不宣', 2),\n",
       " ('得知', 2),\n",
       " ('到任', 2),\n",
       " ('产假', 2),\n",
       " ('表态', 2),\n",
       " ('否认', 2),\n",
       " ('属实', 2),\n",
       " ('发声', 2),\n",
       " ('敲定', 2),\n",
       " ('坚称', 2),\n",
       " ('公开场合', 2),\n",
       " ('所提', 2),\n",
       " ('否决', 2),\n",
       " ('指责', 2),\n",
       " ('伊原', 2),\n",
       " ('来看', 2),\n",
       " ('尽管', 2),\n",
       " ('并非', 2),\n",
       " ('的确', 2),\n",
       " ('潜在', 2),\n",
       " ('看重', 2),\n",
       " ('充满信心', 2),\n",
       " ('前景', 2),\n",
       " ('乐观', 2),\n",
       " ('寄予厚望', 2),\n",
       " ('强劲', 2),\n",
       " ('非常明显', 2),\n",
       " ('有利', 2),\n",
       " ('看中', 2),\n",
       " ('水涨船高', 2),\n",
       " ('倒爷', 2),\n",
       " ('一方面', 2),\n",
       " ('这方面', 2),\n",
       " ('短浅', 2),\n",
       " ('因为', 2),\n",
       " ('想必', 2),\n",
       " ('有失', 2),\n",
       " ('打得', 2),\n",
       " ('自认', 2),\n",
       " ('深感', 2),\n",
       " ('忧心忡忡', 2),\n",
       " ('不解', 2),\n",
       " ('确实', 2),\n",
       " ('这样', 2),\n",
       " ('可是', 2),\n",
       " ('从没', 2),\n",
       " ('挺', 2),\n",
       " ('感觉', 2),\n",
       " ('明白', 2),\n",
       " ('想不到', 2),\n",
       " ('有时候', 2),\n",
       " ('未必', 2),\n",
       " ('看得', 2),\n",
       " ('怎么样', 2),\n",
       " ('也许', 2),\n",
       " ('嘛', 2),\n",
       " ('一点儿', 2),\n",
       " ('形容', 2),\n",
       " ('聊到', 2),\n",
       " ('说实话', 2),\n",
       " ('喜出望外', 2),\n",
       " ('薇', 2),\n",
       " ('庆幸', 2),\n",
       " ('苏轶人', 2),\n",
       " ('汤洁峰', 2),\n",
       " ('不久前', 1),\n",
       " ('库泰萨', 1),\n",
       " ('森访', 1),\n",
       " ('不明智', 1),\n",
       " ('联访', 1),\n",
       " ('广播公司', 1),\n",
       " ('字幕组', 1),\n",
       " ('艾奥瓦', 1),\n",
       " ('消息报', 1),\n",
       " ('郑继永说', 1),\n",
       " ('雅二大', 1),\n",
       " ('兼', 1),\n",
       " ('坚说', 1),\n",
       " ('兼任', 1),\n",
       " ('上海大学', 1),\n",
       " ('北京第二外国语学院', 1),\n",
       " ('管理学', 1),\n",
       " ('毫不讳言', 1),\n",
       " ('海淀园', 1),\n",
       " ('担任', 1),\n",
       " ('海为', 1),\n",
       " ('攻读', 1),\n",
       " ('秦晓雯', 1),\n",
       " ('成绩优秀', 1),\n",
       " ('阿米尔', 1),\n",
       " ('非常高兴', 1),\n",
       " ('历史课', 1),\n",
       " ('感谢', 1),\n",
       " ('自豪', 1),\n",
       " ('骄傲', 1),\n",
       " ('赞美', 1),\n",
       " ('惊叹', 1),\n",
       " ('民间文学', 1),\n",
       " ('直言不讳', 1),\n",
       " ('谦虚', 1),\n",
       " ('盛赞', 1),\n",
       " ('解读', 1),\n",
       " ('明智', 1),\n",
       " ('民法', 1),\n",
       " ('总则', 1),\n",
       " ('决策', 1),\n",
       " ('之问', 1),\n",
       " ('几点', 1),\n",
       " ('部门规章', 1),\n",
       " ('决断', 1),\n",
       " ('看法', 1),\n",
       " ('权威', 1),\n",
       " ('然而', 1),\n",
       " ('如此', 1),\n",
       " ('但', 1),\n",
       " ('为何', 1),\n",
       " ('众所周知', 1),\n",
       " ('印证', 1),\n",
       " ('指向', 1),\n",
       " ('估计', 1),\n",
       " ('推测', 1),\n",
       " ('介入', 1),\n",
       " ('促使', 1),\n",
       " ('结论', 1),\n",
       " ('判断', 1),\n",
       " ('对付', 1),\n",
       " ('意图', 1),\n",
       " ('基于', 1),\n",
       " ('另一方面', 1),\n",
       " ('意愿', 1),\n",
       " ('航空业', 1),\n",
       " ('策略', 1),\n",
       " ('现状', 1),\n",
       " ('当前', 1),\n",
       " ('谨慎', 1),\n",
       " ('审慎', 1),\n",
       " ('短期内', 1),\n",
       " ('不确定性', 1),\n",
       " ('贸易逆差', 1),\n",
       " ('长期', 1),\n",
       " ('还是', 1),\n",
       " ('就是', 1),\n",
       " ('感兴趣', 1),\n",
       " ('选择', 1),\n",
       " ('所以', 1),\n",
       " ('赢家', 1),\n",
       " ('一定', 1),\n",
       " ('也', 1),\n",
       " ('都', 1),\n",
       " ('一点', 1),\n",
       " ('如果', 1),\n",
       " ('首先', 1),\n",
       " ('同样', 1),\n",
       " ('常说', 1),\n",
       " ('高佑思', 1),\n",
       " ('留给', 1),\n",
       " ('爱上', 1),\n",
       " ('情结', 1),\n",
       " ('出身', 1),\n",
       " ('羡慕', 1),\n",
       " ('深知', 1),\n",
       " ('钟爱', 1),\n",
       " ('老一辈', 1),\n",
       " ('熟知', 1),\n",
       " ('聊起', 1),\n",
       " ('模样', 1),\n",
       " ('心中', 1),\n",
       " ('真的', 1),\n",
       " ('想过', 1),\n",
       " ('从来', 1),\n",
       " ('当时', 1),\n",
       " ('既然', 1),\n",
       " ('没想到', 1),\n",
       " ('哭', 1),\n",
       " ('于是', 1),\n",
       " ('奇怪', 1),\n",
       " ('只不过', 1),\n",
       " ('以为', 1),\n",
       " ('很少', 1),\n",
       " ('害怕', 1),\n",
       " ('说出', 1),\n",
       " ('忍不住', 1),\n",
       " ('想起', 1),\n",
       " ('对不起', 1),\n",
       " ('记得', 1),\n",
       " ('妈', 1),\n",
       " ('一脸', 1),\n",
       " ('太棒了', 1),\n",
       " ('谈过', 1),\n",
       " ('嘲笑', 1),\n",
       " ('出走', 1),\n",
       " ('惦记着', 1),\n",
       " ('魔术师', 1),\n",
       " ('不可思议', 1),\n",
       " ('太太', 1),\n",
       " ('害羞', 1),\n",
       " ('老友', 1),\n",
       " ('在我看来', 1),\n",
       " ('极力推荐', 1),\n",
       " ('何许人也', 1),\n",
       " ('预料到', 1),\n",
       " ('不在乎', 1),\n",
       " ('一再强调', 1),\n",
       " ('问起', 1),\n",
       " ('服众', 1),\n",
       " ('非常感谢', 1),\n",
       " ('甘春', 1),\n",
       " ('邓敏', 1),\n",
       " ('黄和逊', 1),\n",
       " ('商报讯', 1),\n",
       " ('严锋', 1),\n",
       " ('韩茜', 1),\n",
       " ('谢晗', 1),\n",
       " ('李秀明', 1),\n",
       " ('李晓渝', 1),\n",
       " ('张代蕾', 1),\n",
       " ('周效政', 1),\n",
       " ('刘帅', 1),\n",
       " ('韩冲', 1),\n",
       " ('科技日报', 1),\n",
       " ('韩梁', 1),\n",
       " ('赵珺', 1),\n",
       " ('李华', 1),\n",
       " ('张侨', 1),\n",
       " ('吴飞座', 1),\n",
       " ('尧', 1),\n",
       " ('王振宏', 1),\n",
       " ('王希', 1),\n",
       " ('姜辰蓉', 1),\n",
       " ('毛鑫', 1),\n",
       " ('庞元元', 1),\n",
       " ('姜潇', 1),\n",
       " ('董小红', 1),\n",
       " ('陆文军', 1),\n",
       " ('薛钦峰', 1),\n",
       " ('洪天牧', 1),\n",
       " ('穆序', 1)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(related_words.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_frequency(word): \n",
    "    return sum(1 for n in news_content if word in n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_frequency('的')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(word):\n",
    "    \"\"\"Gets the inversed document frequency\"\"\"\n",
    "    return math.log10(len(news_content) / document_frequency(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf('的') < idf('小米')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(word, document):\n",
    "    \"\"\"\n",
    "    Gets the term frequemcy of a @word in a @document.\n",
    "    \"\"\"\n",
    "    words = document.split()\n",
    "    \n",
    "    return sum(1 for w in words if w == word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content['content'][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf('银行', news_content[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf('创业板', news_content[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf('创业板')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf('银行')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf('短期')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf('短期', news_content[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_of_a_ducment(document):\n",
    "    words = set(document.split())\n",
    "    \n",
    "    tfidf = [\n",
    "        (w, tf(w, document) * idf(w)) for w in words\n",
    "    ]\n",
    "    \n",
    "    tfidf = sorted(tfidf, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun get_keywords_of_a_ducment(news_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_new_keywords = get_keywords_of_a_ducment(news_content[101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_keywords_of_a_ducment(news_content[101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = wordcloud.WordCloud('/Users/mqgao/Downloads/SourceHanSerifSC-Regular.otf')\n",
    "# we could download the font from https://github.com/Computing-Intelligence/datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(wc.generate_from_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_new_keywords_dict = {w: score for w, score in machine_new_keywords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wc.generate_from_frequencies(machine_new_keywords_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shenzhen_social_news = get_keywords_of_a_ducment(news_content[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shenzhen_social_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "police_mask = np.array(Image.open('/Users/mqgao/Downloads/0034.png_860.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_with_mask = wordcloud.WordCloud(\n",
    "font_path='/Users/mqgao/Downloads/SourceHanSerifSC-Regular.otf', \n",
    "mask=police_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wc.generate_from_frequencies({w:s for w, s in shenzhen_social_news[:20]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud_with_mask.generate_from_frequencies({w:s for w, s in shenzhen_social_news[:20]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Vectorizezd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = TfidfVectorizer(max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "news_content[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 50000\n",
    "sub_samples = news_content[:sample_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorized.fit_transform(sub_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(X[0].toarray()) # get the positions which values are not zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id_1, document_id_2 = random.randint(0, 1000), random.randint(0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content[document_id_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content[document_id_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_of_d_1 = X[document_id_1].toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_of_d_2 = X[document_id_2].toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_choose = random.randint(0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content[random_choose]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(v1, v2): return cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance([1, 1], [2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance(X[random_choose].toarray()[0], X[document_id_1].toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance(X[random_choose].toarray()[0], X[document_id_2].toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content[320]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content[72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(range(10000)), key=lambda i: distance(X[random_choose].toarray()[0], \n",
    "                                      X[i].toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin(38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin(49 & 38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Search Engine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Input: Words\n",
    "Output: Documents\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_search(keywords):\n",
    "    news_ids = [i for i, n in enumerate(news_content) if all(w in n for w in keywords)]\n",
    "    # O(D * w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "naive_search('美军 司令 航母'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input word -> the documents which contain this word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_x = X.transpose().toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_id = vectorized.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_id['今天']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_2_word = {d: w for w, d in word_2_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_2_word[6195]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(np.where(transposed_x[6195])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'美军'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_id['美军']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_id['司令']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_force = set(np.where(transposed_x[7922])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commander = set(np.where(transposed_x[2769])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_force & commander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1, d2, d3 = {1, 2, 3}, {4, 5, 6, 3, 2}, {1, 3, 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import and_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce(and_, [d1, d2, d3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine(query):\n",
    "    \"\"\"\n",
    "    @query is the searched words, splited by space\n",
    "    @return is the related documents which ranked by tfidf similarity\n",
    "    \"\"\"\n",
    "    words = query.split()\n",
    "    \n",
    "    query_vec = vectorized.transform([' '.join(words)]).toarray()[0]\n",
    "\n",
    "    candidates_ids = [word_2_id[w] for w in words]\n",
    "    \n",
    "    documents_ids = [\n",
    "         set(np.where(transposed_x[_id])[0]) for _id in candidates_ids\n",
    "    ]\n",
    "    \n",
    "    merged_documents = reduce(and_, documents_ids)\n",
    "    # we could know the documents which contain these words\n",
    "    sorted_docuemtns_id = sorted(merged_documents, key=lambda i: distance(query_vec, X[i].toarray()))\n",
    "\n",
    "    return sorted_docuemtns_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(vectorized.transform(['美联储 加息 次数']).toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"新华社洛杉矶４月８日电（记者黄恒）美国第三舰队８日发布声明说，该舰队下属的“卡尔·文森”航母战斗群当天离开新加坡，改变原定驶往澳大利亚的任务计划，转而北上，前往西太平洋朝鲜半岛附近水域展开行动。\\n　　该舰队网站主页发布的消息说，美军太平洋司令部司令哈里·哈里斯指示“卡尔·文森”航母战斗群向北航行。这一战斗群包括“卡尔·文森”号航空母舰、海军第二航空队、两艘“阿利·伯克”级导弹驱逐舰和一艘“泰孔德罗加”级导弹巡洋舰。\\n　　“卡尔·文森”号航母的母港位于美国加利福尼亚州的圣迭戈，今年１月初前往西太平洋地区执行任务，并参与了日本及韩国的军事演习。\\n　　美国有线电视新闻网援引美国军方官员的话说，“‘卡尔·文森’号此次行动是为了对近期朝鲜的挑衅行为作出回应”。（完）\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"美国有线电视新闻网援引美国军方官员的话说\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'(新闻|官员)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.compile(pat).sub(repl=\"**\\g<1>**\", string=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_pat(query):\n",
    "    return re.compile('({})'.format('|'.join(query.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_query_pat('美军 司令 航母')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_keywords(pat, document):\n",
    "    return pat.sub(repl=\"**\\g<1>**\", string=document) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_keywords(get_query_pat('美军 司令 航母'), content['content'][22987])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_with_pretty_print(query):\n",
    "    candidates_ids = search_engine(query)\n",
    "    for i, _id in enumerate(candidates_ids):\n",
    "        title = '## Search Result {}'.format(i)\n",
    "        c = content['content'][_id]\n",
    "        c = highlight_keywords(get_query_pat(query), c)    \n",
    "        \n",
    "        display(Markdown(title + '\\n' + c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine_with_pretty_print('春节 假期')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "search_engine('美联储 加息 次数')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content['content'][2189]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "preprocessing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genearte_random_website():\n",
    "    return ''.join([random.choice(ascii_uppercase) for _ in range(random.randint(3, 5))]) + '.'  + random.choice(['com', 'cn', 'net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genearte_random_website()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = [genearte_random_website() for _ in range(25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(websites, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_connection = {\n",
    "    websites[0]: random.sample(websites, 10),\n",
    "    websites[1]: random.sample(websites, 5),\n",
    "    websites[3]: random.sample(websites, 7),\n",
    "    websites[4]: random.sample(websites, 2),\n",
    "    websites[5]: random.sample(websites, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_network = nx.graph.Graph(website_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3,figsize=(12,12))\n",
    "nx.draw_networkx(website_network, font_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(nx.pagerank(website_network).items(),key=lambda x: x[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
